[
  {
    "objectID": "stock_prices.html",
    "href": "stock_prices.html",
    "title": "Stock Price Tracker Application",
    "section": "",
    "text": "This project was built using Plotly Dash, a dashboard framework that is built on Flask. Through user input, a stock ticker and a time period can be specified, for which the application will:\n\nExtract the data from the Yahoo Finance API\nTransform the data appropriately using Pandas\nCreate a time series graphical representation of the data with Plotly\nLoad both the plot and the table into the dashboard.\n\nFeel free to explore the app running with Render:\nhttps://stock-price-dashboard.onrender.com/\nIt may take a minute or two to spin up in the server.\nIn the meantime, check out the code I wrote to create the project:\n\nfrom dash import Dash, html, dash_table, dcc, Input, Output, State, no_update\nimport dash_bootstrap_components as dbc\nimport pandas as pd\nimport plotly_express as px\nimport yfinance as yf\nfrom datetime import datetime\n\n\n'''\nSTRUCTURE OF THE APP\n1. defining functions to use to retrieve and display data and create plots\n2. initializing the app and app layout\n3. app callback, Inputs/Outputs, and callback function\n\nDash inherits a certain format for code, as far as my research indicates, \nso the app itself is best defined entirely in the main function at the very least.\nMy research indicates that global scope would be even better and more straightforward,\nbut using main() seems to work out okay. Function defaults are the intended\nvalues upon app startup.\n\nDocumentation:\nhttps://finance.yahoo.com/lookup/\nhttps://pypi.org/project/yfinance/\nhttps://dash.plotly.com/\nhttps://plotly.com/python/plotly-express/\n\n'''\n\n#Functions for use within the app\n\ndef get_ticker_object(ticker:str='^GSPC'):\n    #initializes object; this object is used in two different functions so it is more efficient to have it on larger scope in its own function\n    ticker_object = yf.Ticker(ticker)\n    return ticker_object\n\ndef get_ticker_history_df(ticker_object, time:str = 'ytd'):\n   '''This code uses yfinance module to get historical data from the Yahoo \n   Finance API. \n   \n   Ticker: Gets data for S&P 500 using ticker '^GSPC' by default.\n   Takes any valid ticker identifier. \n   \n   Time: default = 'ytd'; \n   possible: ['1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max']\n   '''\n\n   if time != '1d':\n    #gets history given time frame\n      history = ticker_object.history(time)\n       #date is current index; resets index\n      df = pd.DataFrame(history).reset_index()\n      df= df.sort_values('Date', ascending = False)\n   elif time == '1d':\n      history = ticker_object.history(time, interval = '1m')\n      df = pd.DataFrame(history).reset_index()\n      df= df.sort_values('Datetime', ascending = True)\n\n   \n       \n       #more descriptive column names\n   old = df.columns[1:5]\n   new = [f'{col} (USD)' for col in df.columns[1:5]]\n   old_new_dict = dict(zip(old, new))\n   df = df.rename(columns = old_new_dict)\n\n   if time == '1d':\n      df['Datetime'] = pd.to_datetime(df['Datetime'])\n      df['Time'] = df['Datetime'].dt.time\n       \n\n    \n    #returns df\n   return df\n\ndef get_stock_name(ticker_object):\n    info = ticker_object.info\n\n    if info != {'trailingPegRatio': None}: #this is the output when the query can't find the given stock\n        return info['longName']\n    else:\n        return 'Stock Not Found'\n\n#makes fig when time != '1d'; '1d' takes different time formatting on x-axis\ndef get_ticker_history_fig(df, name):\n    fig = px.line(\n        df,\n        x = 'Date',\n        y = 'Close (USD)',\n    ).update_layout(\n        xaxis_title = 'Date',\n        yaxis_title = 'Daily Closing Price (USD)',\n        title = f'Closing Prices of Stocks over Time- {name}',\n        template = 'plotly_dark'\n    )\n\n    return fig\n\n#makes fig when time == '1d' with hr/min on x-axis\ndef get_ticker_history_fig_day_price(df, name):\n    fig = px.line(\n        df,\n        x = 'Time',\n        y = 'Close (USD)'\n    ).update_layout(\n        xaxis_title = '',\n        yaxis_title = 'Price',\n        title = f\"Today's Price- {name}\",\n        template = 'plotly_dark'\n    ).update_xaxes(\n        tickformat='%I:%M %p' \n        \n        '''\n        can't figure out why this doesn't work; I want to format\n        the time ticks differently but it won't work properly\n        '''\n    )\n\n    return fig\n\n#---------------------------------------------------------------------\n\n'''APP STARTS HERE'''\n\n#initializes Dash object 'app'; however, an initial callback is still made\napp = Dash(__name__, external_stylesheets=[dbc.themes.CYBORG])\nserver = app.server\n\nticker_object = get_ticker_object('^GSPC')\nname = get_stock_name(ticker_object)\ndf = get_ticker_history_df(ticker_object)\nfig = get_ticker_history_fig(df, name)\n\n\n#initializes layout of page\napp.layout = html.Div([\n    html.Header([\n        html.H1('Stock Price Tracker', id = 'app-title', style = {'marginBottom': 5}),\n    html.H2(str(datetime.now().date().__format__('%B %d, %Y')), id = 'date-header', style = {'marginTop': 5})\n], id = 'header'),\n    html.Div([html.P(\"Provide a ticker and select a time frame for data display. For example, FAANG tickers are 'FB', 'AMZN', 'AAPL', 'NFLX'; S&P 500 is '^GSPC'. For more info, visit the following site: \"), html.P(html.A(' Yahoo Finance Lookup', href = 'https://finance.yahoo.com/lookup/', target='_blank', id = 'yf-link', style= {'color': 'white'}))], \n             style={'marginBottom':'15px'}\n             ),\n    html.P('Select stock (press Enter to update):'),\n    dcc.Input(\n        id='stock-input',\n        type = 'text',\n        value = '^GSPC',\n        placeholder= 'Give any valid ticker (ex. ^GSCP is default):',\n        n_submit = 1\n    ),\n    html.P('Select timeframe:'),\n    dcc.Dropdown(\n        id = 'time-input',\n        options = ['1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max'],\n        value = 'ytd', \n        clearable = False\n    ),\n    html.Br(),\n    html.Hr(),\n    dcc.Graph(id='time-series-chart', style={'marginBottom': 20, 'marginTop': 20}),\n    html.Hr(style = {'marginBottom': 20}),\n    dash_table.DataTable(id= 'data-table', data=df.to_dict('records'), page_size = 10, style_header={'backgroundColor': 'rgb(30, 30, 30)', 'color': 'white'}, style_data={'backgroundColor': 'rgb(50, 50, 50)', 'color': 'white'})\n])\n\n#callback to the app; Input tracks input changes to produce Output\n@app.callback(\n[Output('data-table', 'data'), Output('time-series-chart', 'figure')],\n[Input('stock-input', 'n_submit'), Input('time-input', 'value')],\n[State('stock-input', 'value')]\n)\n\n#callback function associated with new ticker or time frame\ndef update(n_submit, time_input, stock_input,):\n    if (stock_input and n_submit &gt; 0) or time_input: #if stock changes and 'Enter', or time input changes\n        try:\n            ticker_object = get_ticker_object(str(stock_input))\n            df = get_ticker_history_df(ticker_object, time = str(time_input))\n            name = get_stock_name(ticker_object)\n            if time_input != '1d':\n                fig = get_ticker_history_fig(df, name)\n            elif time_input == '1d':\n                fig = get_ticker_history_fig_day_price(df, name)\n        except:\n            df = pd.DataFrame(columns= ['Close (USD)', 'Date'])\n            return df.to_dict('records'), get_ticker_history_fig(df, 'Stock Not Found')\n        return df.to_dict('records'), fig\n    \n    return no_update, no_update #won't update without new data\n'''\n    to_dict('records') is necessary because Dash needs a JSON object\n    these functions inherit the form of the callback (specifically the order);\n    'stock-input' becomes stock_input arg, time-input becomes time_input arg\n    output df.to_dict('records') becomes Output('data-table'), fig becomes Output('time-series-chart')\n'''\n     #debug=True for debugging functionality\n\nif __name__ == '__main__':\n    app.run_server(debug=False)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Signature Projects",
    "section": "",
    "text": "R-markdown analysis project using R: Uncovered trends in poverty and race in relation to crime rates using official US Census data. Showcased abilities in hypothesis testing, linear regression, ANOVA, model validation, API data extraction, exploratory data analysis, and data visualization (ggplot).\n\n\n\n\n\n\n\nPython Application: implemented and automated ETL process to retrieve data from Yahoo Finance API via yfinance python module, appropriately transforming resulting data with pandas, and loading resulting data frame into a Plotly Dash app graphically and tabularly: Showcase abilities with Python, Dash, Plotly, Pandas, yfinance, HTML, and CSS.\n\n\n\n\n\n\n\nThese projects are intended to showcase the abilities I have developed in three main areas:\n1. Python\nMy experience with python consists of the following:\n\nPandas\nNumPy\nScikit-learn\nAPI data extraction\nRequests\nData wrangling\nData visualization\nData manipulation\nExploratory data analysis\nmachine learning\nReading/writing to different file types\n.csv\n.json\nSQLite\nJupyter notebooks\nQuarto markdown\n\n2. R\nMy experience with R consists of the following:\n\nGGPlot\nMosaic\nTidyverse\nStatistics\nData wrangling\nData visualization\nData manipulation\nExploratory data analysis\nreading various file types\nR-markdown\n\n3. Statistics\nMy experience with statistics involves using R for data manipulation and analysis for the following techniques:\n\nHypothesis testing\nT-tests\nWilcoxon tests\nMann-Whitney tests\nAnalysis of variance (ANOVA)\nKruskal-Wallis tests\nChi-squared tests\nSimple and multiple linear regression\nSimple and multiple logistic regression\nPermutation testing\nModel validation"
  },
  {
    "objectID": "projects.html#socioeconomic-inequality-statistical-analysis",
    "href": "projects.html#socioeconomic-inequality-statistical-analysis",
    "title": "Signature Projects",
    "section": "",
    "text": "R-markdown analysis project using R: Uncovered trends in poverty and race in relation to crime rates using official US Census data. Showcased abilities in hypothesis testing, linear regression, ANOVA, model validation, API data extraction, exploratory data analysis, and data visualization (ggplot)."
  },
  {
    "objectID": "projects.html#project-2-stock-price-tracking-application",
    "href": "projects.html#project-2-stock-price-tracking-application",
    "title": "Signature Projects",
    "section": "",
    "text": "Python Application: implemented and automated ETL process to retrieve data from Yahoo Finance API via yfinance python module, appropriately transforming resulting data with pandas, and loading resulting data frame into a Plotly Dash app graphically and tabularly: Showcase abilities with Python, Dash, Plotly, Pandas, yfinance, HTML, and CSS."
  },
  {
    "objectID": "projects.html#skills-developed",
    "href": "projects.html#skills-developed",
    "title": "Signature Projects",
    "section": "",
    "text": "These projects are intended to showcase the abilities I have developed in three main areas:\n1. Python\nMy experience with python consists of the following:\n\nPandas\nNumPy\nScikit-learn\nAPI data extraction\nRequests\nData wrangling\nData visualization\nData manipulation\nExploratory data analysis\nmachine learning\nReading/writing to different file types\n.csv\n.json\nSQLite\nJupyter notebooks\nQuarto markdown\n\n2. R\nMy experience with R consists of the following:\n\nGGPlot\nMosaic\nTidyverse\nStatistics\nData wrangling\nData visualization\nData manipulation\nExploratory data analysis\nreading various file types\nR-markdown\n\n3. Statistics\nMy experience with statistics involves using R for data manipulation and analysis for the following techniques:\n\nHypothesis testing\nT-tests\nWilcoxon tests\nMann-Whitney tests\nAnalysis of variance (ANOVA)\nKruskal-Wallis tests\nChi-squared tests\nSimple and multiple linear regression\nSimple and multiple logistic regression\nPermutation testing\nModel validation"
  },
  {
    "objectID": "projects.html#relevant-coursework",
    "href": "projects.html#relevant-coursework",
    "title": "Signature Projects",
    "section": "Relevant Coursework",
    "text": "Relevant Coursework\nThis section holds smaller projects completed as part of university course assignments.\n\nData Science ProgrammingIntermediate StatisticsIntroduction to Databases\n\n\n\nData Science Programming\nThis class focused on using Python for data wrangling, cleaning, visualization, analysis, and introductory machine learning.\n\n\n\n\nIntermediate Statistics\nThis class focused on using R for hypothesis testing, introductory regression techniques, model validation, and data wrangling, cleaning, and visualization.\n\n\n\n\nIntroduction to Databases\nThis class introduced relational databases. It focused on mapping entity-relationship diagrams (ERDs), writing SQL queries, and emphasized utilizing standard industry best practices for data storage and use."
  },
  {
    "objectID": "projects.html#data-science-programming-1",
    "href": "projects.html#data-science-programming-1",
    "title": "Signature Projects",
    "section": "Data Science Programming",
    "text": "Data Science Programming\nThis class focused on using Python for data wrangling, cleaning, visualization, analysis, and introductory machine learning."
  },
  {
    "objectID": "projects.html#intermediate-statistics-1",
    "href": "projects.html#intermediate-statistics-1",
    "title": "Signature Projects",
    "section": "Intermediate Statistics",
    "text": "Intermediate Statistics\nThis class focused on using R for hypothesis testing, introductory regression techniques, model validation, and data wrangling, cleaning, and visualization."
  },
  {
    "objectID": "projects.html#introduction-to-databases-1",
    "href": "projects.html#introduction-to-databases-1",
    "title": "Signature Projects",
    "section": "Introduction to Databases",
    "text": "Introduction to Databases\nThis class introduced relational databases. It focused on mapping entity-relationship diagrams (ERDs), writing SQL queries, and emphasized utilizing standard industry best practices for data storage and use."
  },
  {
    "objectID": "projects.html#minor-projects",
    "href": "projects.html#minor-projects",
    "title": "Signature Projects",
    "section": "Minor Projects",
    "text": "Minor Projects\nSometimes I complete projects that arenâ€™t very large or significant, and yet still showcase my abilities in some way. This is where you can find these projects. They will be sorted by skills and technologies used."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Portfolio!",
    "section": "",
    "text": "Hello! My name is Andrew Belz. Since 2023, I have been a student at BYU-Idaho studying data science. Starting in January 2025, I will be pursuing a degree in statistics with an emphasis in data science at Brigham Young University in Provo, Utah.\n\n\n\n\nMy Resume Projects GitHub LinkedIn Email Me Call Me\n\n\n\nI grew up in southern California, in a town called Simi Valley. I am the oldest of four; I have two younger sisters, and one younger brother. I love chess, cooking, and motorcycles! I will happily have a lengthy conversation about any of those topics, anywhere, anytime.\n\n\n\nI created this site for multiple purposes. First and foremost, I want to get my projects that I have worked on out there so that I can properly showcase my abilities! I love coming up with new projects to do, and I am constantly pushing myself beyond my limits. I love to experiment with new ideas and technologies so that I can grow as a professional and a person. I love to learn about math, science, and technology because that is what is driving innovation today.\nI sincerely hope that this portfolio adequately represents my passion!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome to my Portfolio!",
    "section": "",
    "text": "I grew up in southern California, in a town called Simi Valley. I am the oldest of four; I have two younger sisters, and one younger brother. I love chess, cooking, and motorcycles! I will happily have a lengthy conversation about any of those topics, anywhere, anytime."
  },
  {
    "objectID": "index.html#about-this-site",
    "href": "index.html#about-this-site",
    "title": "Welcome to my Portfolio!",
    "section": "",
    "text": "I created this site for multiple purposes. First and foremost, I want to get my projects that I have worked on out there so that I can properly showcase my abilities! I love coming up with new projects to do, and I am constantly pushing myself beyond my limits. I love to experiment with new ideas and technologies so that I can grow as a professional and a person. I love to learn about math, science, and technology because that is what is driving innovation today.\nI sincerely hope that this portfolio adequately represents my passion!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "ANDREW BELZ",
    "section": "",
    "text": "ANDREW BELZ\n\nGitHub LinkedIn andrewbelzjr@gmail.com (805) 624-4436\n\n\nPassionate aspiring data scientist always looking for new challenges. Technically adept, with both mathematical and programming skills in statistics, Python, R, and SQL. Flexible and hard-working team player.\n\n\nProject Experience Highlights\n\n\nSocioeconomic Inequality Statistical Analysis\n\n\nBYU-Idaho\n\nAnalysis Project using R: Uncovered trends in poverty and race in relation to crime rates using official US Census data. Showcased abilities in hypothesis testing, linear regression, ANOVA, model validation, API data extraction, exploratory data analysis, and data visualization (ggplot).\n\n\nStock Price Tracking Application\n\n\nBYU-Idaho\n\nPython Application: implemented and automated ETL process to retrieve data from Yahoo Finance API via yfinance python module, appropriately transforming resulting data with pandas, and loading resulting data frame into a Plotly Dash app graphically and tabularly: Showcase abilities with Python, Dash, Plotly, Pandas, yfinance, HTML, and CSS.\n\n\n\nEducation\n\nB.S. in Statistics, Emphasis in Data Science, Minor in Mathematics\n\n\nExpected December 2027\n\n\n\n Expected from Brigham Young University (Provo, UT)\nWill attend Winter 2025 - Fall 2027\n\n\n\n\nBrigham Young University-Idaho\nAttending Fall 2023 - Fall 2024\n4.0 GPA (on 4-point scale)\nRelevant Coursework: Data Science Programming, Intermediate Statistics, Introduction to Databases\n\n\n\n\nWork Experience\n\nSolar Technician\nMitchell Security, Inc., Bakersfield, CA\nAugust 2022 - August 2023\n\nInstalled residential solar energy systems in a team of 6\nImplemented and utilized monitoring software for energy systems to diagnose and troubleshoot performance\nCommunicated with clients to deliver focused solutions\nAdhered to strict build protocol and regulations to promote strict quality control\n\n\n\nUniversity Exam Proctor\nBYU-Idaho, Rexburg, ID\nMay 2024 - Present\n\nAdminister and manage tests of dozens of students daily\nComply with standard operating procedures to protect testing integrity and accurate file management\nInteract with database front end interface to manage student and test information\n\n\n\n\n\nSkills\nPython: Pandas, NumPy, Scikit-learn, Plotly Express | R: ggplot, mosaic, tidyverse, R-markdown | SQL: SQLite, MySQL, ERDs | Hypothesis testing | Statistical modeling | Data visualization | Data wrangling | Effective communication | Team collaboration | Problem-solving | Quality control\n\n\nClick here to download this resume\n\n\n\n\n\n\n\n Back to top"
  }
]